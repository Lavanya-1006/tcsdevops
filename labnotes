Lab 1
 

Jenkins master setup: https://www.jenkins.io/doc/book/installing/linux/#red-hat-centos
 
sudo wget -O /etc/yum.repos.d/jenkins.repo  https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
yum update

# Add required dependencies for the jenkins package
sudo yum install fontconfig java-21-openjdk
sudo yum install jenkins
sudo systemctl daemon-reload
systemctl status jenkins
systemctl enable jenkins
systemctl start jenkins
systemctl status jenkins
 

------------------------
Lab2
1) In the browser : localhost:8080

2) Go to terminal and type

cat /var/lib/jenkins/secrets/initialAdminPassword

3) get the password and login

4) Install suggested plugin

 
 Lab 3:
Lab 2 https://www.openwriteup.com/?page_id=1115

node {
// Define the label of the agent where you want to run the pipeline
//label 'mylabel'
stage('Checkout') {
// Checkout code from the Git repository
sh 'echo checking out'
}
stage('Build') {
// Build the Java application (replace with your build commands)
sh 'java --version'
}
stage('Deploy') {
// Deploy the application (replace with your deployment commands)
sh 'echo "Deploying the application"'
}
}


Plugin: pipeline stage view

-----------------

Lab 4 : Decalartive pipeline :https://www.openwriteup.com/?page_id=1115 (Lab 3)

pipeline {
agent any
stages {
  stage('Checkout') {
    steps {
// Checkout code from the Git repository
     sh 'echo checking out'
   }
  }
stage('Build') {
  steps {
// Build the Java application (replace with your build commands)
  sh 'javac -version'
 }
}
stage('Deploy') {
  steps {
  // Deploy the application (replace with your deployment commands)
   sh 'echo "Deploying the application"'
   }
  }
 } 
} 

Lab 5: Assignment
    
Go in your git repo “myrepo”
Create a “Jenkinsfile”
Copy the code from lab 3
checkin the code to github (using git add,commit and push
Create a new pipeline and scripting block dropdown, select “pipeline with SCM”
Mention your git repo and save .
Run the build


--------------------------------------
27/05

git add,commit and push using  ssh

https://www.openwriteup.com/?page_id=1075

Do the git init, git add, git commit
 

LAb2

https://www.openwriteup.com/?page_id=1081

Lab3:

dnf install maven -y

Lab 1: https://www.openwriteup.com/?page_id=1135

Lab 2: https://www.openwriteup.com/?page_id=1135

Lab 4: https://www.openwriteup.com/?page_id=1135

Lab https://www.openwriteup.com/?page_id=1073 (lab 17)


---------------------------
28/05

https://www.sonarsource.com/products/sonarcloud/

Lab https://www.openwriteup.com/?page_id=1223
 
 
vi /etc/sudoers

jenkins ALL=(ALL) NOPASSWD: ALL
 
 pipeline {
agent any

stages {
stage('chckout scm') {
steps {
checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/hellokaton/java11-examples.git']])
}
}
stage('Compiling and Running Test Cases') {
steps {
sh 'mvn clean'
sh 'mvn compile'
sh 'mvn test'
}
}
stage('Generating a Cucumber Reports') {
steps {
script {
// Run Cucumber tests and generate reports
sh 'mvn verify'
}
}
}
stage('Creating Package') {
steps {
sh 'mvn package'
}
}
stage('adding genrerate report'){
steps {
sh 'mvn verify'
}
}
stage('Install sonarqube cli') {
steps {
// Step to install SonarQube CLI
sh 'sudo wget -O sonar-scanner.zip https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-5.0.1.3006-linux.zip'
sh 'sudo unzip -o -q sonar-scanner.zip'
sh 'sudo rm -rf /opt/sonar-scanner'
sh 'sudo mv --force sonar-scanner-5.0.1.3006-linux /opt/sonar-scanner'
sh 'sudo sh -c \'echo "#/bin/bash \nexport PATH=\\\"$PATH:/opt/sonar-scanner/bin\\\"" >/etc/profile.d/sonar-scanner.sh\''
sh 'sudo chmod +x /opt/sonar-scanner/bin/sonar-scanner'
sh '. /etc/profile.d/sonar-scanner.sh'
}
}

stage('Analyzing Code Quality') {
steps {
// Step to analyze code quality with SonarQube
sh '/opt/sonar-scanner/bin/sonar-scanner -Dsonar.projectKey=owtest23_myproj -Dsonar.organization=owtest23 -Dsonar.qualitygate.wait=true -Dsonar.qualitygate.timeout=300 -Dsonar.sources=src/main/java/ -Dsonar.java.binaries=target/classes -Dsonar.host.url=https://sonarcloud.io -Dsonar.login=cd718b63ea3b0b88b10541ff23b08430789260e1'
}
}
}
}
---------------------------
install dcoekr 
LAb 1 : https://www.openwriteup.com/?page_id=785

Lab2:

docker search nginx
docker pull nginx
docker images

docker pull mysql:innovation
docker images


docker pull mysql:innovation
docker images

#To check the running container
 
docker ps

docker run -d --name mycontainer nginx

docker ps

docker stop mycontainer

docker ps

docker ps -a

docker start mycontainer

docker ps

-------------------------------

docker inspect mycontainer

'---------------
29/05

Lab

dokken/centos-8

docker pull dokken/centos-8

docker images
docker run -dit --name centos dokken/centos-8 bash
docker ps --nam
docker exec -it centos ip addr
docker exec -it centos ps

windowsn containre
docker pull mcr.microsoft.com/windows/nanoserver:ltsc2025


--------------------------------------

Lab 2

1) docker pull debian:stable
2) docker run -dit --name testdeb debian:stable
3) docker exec -it testdeb ls /var/www/html

#error file not found

4) vi dockerfile and copy below content

FROM debian:stable
LABEL authors="amit"
RUN apt-get update && apt-get install -y --force-yes apache2
EXPOSE 80

5) docker build -t mydeb .
6) docker images
7) docker run -dit --name testweb mydeb
8) docker exec -it testweb ls /var/www/html

Lab3:

docker pull ubuntu:latest
docker run -dit --name addcopy ubuntu:latest
docker exec -it addcopy ls /app

touch file.txt
vi dockerfileadd and copy below code, save it

FROM ubuntu:latest
# Set a working director inside container
WORKDIR /app
#Copy file from current directory
COPY file.txt /app/
#ADD a directory from the host
#ADD /home/lab-user/amit /app/
ADD https://github.com/amitopenwriteup/my_proj/blob/master/k8s-setup.txt /app/
#Till this point copy it in docker file

docker build -t ubuntuimage -f dockerfileadd  .

docker images

docker run -dit --name addcopytest ubuntuimage
docker exec -it addcopytest ls /app

-----------------------

CMD

#Create date.sh file and copy below script

#!/bin/sh
echo `date` $@ >> log.txt;
cat log.txt;
#save date.sh script

chmod 777 date.sh

#Create a dockercmd file

FROM alpine
ADD date.sh /
CMD ["/date.sh", "container started"]

docker build -t cmdtest -f dockercmd .
docker run cmdtest


-------------------


30/05

https://www.openwriteup.com/?page_id=825 (LAb1 to Lab4 ) : RUN CMD and ENTRYPOINT

LAb2:
signup using github

https://hub.docker.com/



Lab3:
https://www.openwriteup.com/?page_id=838

Push the image

Volume lab:

docker stop registry
docker rm registry

# Refresh the broswer, it will give error page not found

http://localhost:5000/v2/_catalog

docker run -d -p 5000:5000 --restart=always --name registry registry:2

AGain open the browser the below link and check
http://localhost:5000/v2/_catalog

docker stop registry
docker rm registry

----
create a volume

docker volume create datavol
docker inspect datavol
docker run -d -p 5000:5000 --restart=always -v datavol:/var/lib/registry  --name registry registry:2
docker push localhost:5000/my-image
docker stop registry
docker rm registry

   docker run -d -p 5000:5000 --restart=always -v datavol:/var/lib/registry  --name registry registry:2
AGain open the browser the below link and check
http://localhost:5000/v2/_catalog

docker stop registry
docker rm registry

----------map your own volume

mkdir /root/test

docker run -d -p 5000:5000 --restart=always -v /root/test:/var/lib/registry  --name registry registry:2

docker push localhost:5000/my-image
docker stop registry
docker rm registry

docker run -d -p 5000:5000 --restart=always -v /root/test:/var/lib/registry  --name registry registry:2

AGain open the browser the below link and check
http://localhost:5000/v2/_catalog


Lab 4:

#Create the alpine-net network. You do not need the --driver bridge flag since it's the default, but this example shows how to specify it.

docker network create --driver bridge alpine-net

docker network ls

#inspect

docker network inspect alpine-net

Create your four containers. Notice the --network flags. You can only connect to one network during the docker run command, so you need to use docker network connect afterward to connect alpine4 to the bridge network as well.

docker run -dit --name alpine1 --network alpine-net alpine ash

docker run -dit --name alpine2 --network alpine-net alpine ash

docker run -dit --name alpine3 alpine ash

docker run -dit --name alpine4 --network alpine-net alpine ash

docker network connect bridge alpine4

#check all container

docker container ls

docker network inspect bridge


docker network inspect alpine-net


#Ping test

docker container attach alpine1
ping -c 2 alpine2
ping -c 2 alpine4

#It should fail for alpine 3, why?
ping -c 2 alpine3

#Stop and delete

docker container stop alpine1 alpine2 alpine3 alpine4

docker container rm alpine1 alpine2 alpine3 alpine4

docker network rm alpine-net


LAb 5: cicd for dockerfile

https://www.openwriteup.com/?page_id=1350


sudo usermod -a -G docker jenkins
sudo chown jenkins:docker /var/run/docker.sock
sudo chmod 660 /var/run/docker.sock

From Manage plugin install docker pipeline plugin
Create a pipeline And select option “pipeline with scm”
Provide this git link
https://github.com/amitopenwriteup/cicd.git
Branch : master
Jenkinsfile name: Jenkinsfile


---------------------
02/06

Install docker in second vm

install dcoekr 
LAb 1 : https://www.openwriteup.com/?page_id=785


yum remove podman*

yum remove runc* containerd* buildah*

sudo yum install -y yum-utils

sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

systemctl enable docker


systemctl start docker

systemctl status docker

Lab 2:

1) note down the ip address of second vm

ip addr|grep ens

2) First .1 linux vm

ssh root@ipaddrsss
#password is root123

Run step1 in both the linux vm

https://www.openwriteup.com/?page_id=866

step2 : Execute in both vms

https://v1-31.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

step 3:
 master .1

sudo firewall-cmd --permanent --add-port=6443/tcp 
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp 
sudo firewall-cmd --permanent --add-port=10251/tcp 
sudo firewall-cmd --permanent --add-port=10259/tcp 
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=4789/udp
sudo firewall-cmd --reload

worker node /slave 2
sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=30000-32767/tcp
sudo firewall-cmd --permanent --add-port=4789/udp
sudo firewall-cmd --reload
Run on both nodes

swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

sh -c "containerd config default > /etc/containerd/config.toml"
sed -i 's/ SystemdCgroup = false/ SystemdCgroup = true/' /etc/containerd/config.toml
systemctl restart containerd.service
systemctl restart kubelet.service
systemctl enable kubelet.service


step 4 only on master

kubeadm init --pod-network-cidr=10.244.0.0/16


Step5: Runs only in master node
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install the network plugin in master node
step 6: only in master
 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

step 7 verify

sudo firewall-cmd --reload

kubectl get nodes


step 8 on worker node

on the master

kubeadm token create --print-join-command

outp run the join command run in workernode
---------------------

Lab9
 create pod
vi testpod.yaml

copy and paste:
apiVersion: v1
kind: Pod
metadata:
  name: testpod
spec:
  containers:
  - name: firstcontainer
    image: nginx            


kubectl create -f testpod.yaml

kubectl get pods
kubectl describe pods testpod
kubectl get pods -o wide

Check taint
kubectl get nodes

kubectl describe nodes localhost.localdomain|grep Taint


03/06

kubectl get nodes
kubectl get pods

Lab 1:

rm -rf k8s

 git clone https://github.com/amitopenwriteup/k8s.git

cd k8s
kubectl create -f multicont.yaml
kubectl get pods
kubectl describe pod multi-container-pod
kubectl exec multi-container-pod -c main-container -- date
kubectl exec multi-container-pod -c sidecar-container -- ls
kubectl delete pod multi-container-pod

LAb 2
kubectl create ns labs

kubectl describe ns labs

kubectl get ns labs -o yaml

kubectl delete ns labs

Lab 3


cd k8s
cat devpod.yaml
kubectl create ns dev
kubectl create -f devpod.yaml
kubectl get pods --namespace=dev
kubectl describe pod busybox-pod --namespace=dev
kubectl delete pod busybox-pod --namespace=dev


Lab: 4

 kubectl create -f repc.yaml
kubectl get rc
kubectl describe rc hello-dep
kubectl get pods --show-labels
kubectl get pods
kubectl delete pods <provide one of the replica pod name?
kubectl get pods --show-labels
# for deleting rc
kubectl delete -f repc.yaml


Lab 5


kubectl create -f dep.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment nginx-deployment
#For deleting the deployment
kubectl delete deployment nginx-deployment


Lab 6

kubectl create -f ru.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment hello-rcp
# update the image
 kubectl set image deployment/hello-rcp hello-dep=nginx:1.16.1
# Check rollout status
kubectl rollout status deployment/hello-rcp
#Rollout history
kubectl rollout history deployment/hello-rcp
#roll back
 kubectl rollout undo deployment/hello-rcp --to-revision=1
#scaleup
 kubectl scale deployment/hello-rcp --replicas=10

kubectl rollout history deployment hello-rcp --revision=0
kubectl rollout history deployment hello-rcp --revision=2
#delete
 kubectl delete deployment.apps/hello-rcp

LAb7 

Recreate strategy

kubectl create -f rc.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment hello-dep
# update the image
kubectl set image deployment/hello-dep hello-dep=nginx:1.16.1
# Check rollout status
kubectl rollout status deployment/hello-dep
#Rollout history
kubectl rollout history deployment/hello-dep
#roll back
kubectl rollout undo deployment/hello-dep --to-revision=1
#scaleup
kubectl scale deployment/hello-dep --replicas=10
#delete
kubectl delete deployment.apps/hello-dep


--------------------
04/06

git clone https://github.com/amitopenwriteup/k8s.git
cd k8s

Lab1

kubectl create -f svc.yaml
kubectl get svc
kubectl get ep
kubectl describe svc nginx-service-rc
kubectl create -f dep.yaml
kubectl get pods -o wide
kubectl get ep

##Delete the service
kubectl delete -f svc.yaml
kubectl detete -f dep.yaml


-------------LAb2

kubectl create -f dep.yaml
kubectl create -f Nodeport.yaml
kubectl get svc
kubectl get ep

#Note down the worker node (join)

ip addr|grep ens

#Open the ip in the broser

http://ipaddr:30007

#Delete the nodeport service
kubectl delete svc my-service


Lab 3-5

kubectl create -f pv.yaml
kubectl get pv
kubectl describe pv pv-volume-2

kubectl create -f pvc.yaml
kubectl get pvc
kubectl get pv


#Modify the pvpod.yaml, claim name: pvc-claim-3

kubectl create -f pvpod.yaml
kubectl get pod
kubectl exec mypod -it -c myfrontend -- /bin/bash
cd /var/www/html
#create a file using below cmd
touch a
#exit from the pod:
exit
#check the file created in worker node
ls /mnt/data/
--------------

Lab 6 : Dyanmic provisioning

#NFS storage 

#For rocky linux
sudo dnf install -y nfs-utils

#Making a directory in host machine where PersistentVolumeClaim (PVC) will be created

sudo mkdir /srv/nfs/kubedata -p
#Now we will edit the exports file and add the directory 
#which we created earlier step in order to export it into the remote machine

sudo vi /etc/exports

/srv/nfs/kubedata    *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)

sudo systemctl enable nfs-server
sudo systemctl start nfs-server
sudo systemctl status nfs-server
exportfs

sudo exportfs -rav


Master only run below command

dnf install nfs-utils -y

#Install NFS client packages in nodes
wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz
tar -zxvf helm-v3.13.2-linux-amd64.tar.gz
mv linux-amd64/helm /bin/helm

#Add the stable repository in Helm repo and update
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner

helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=10.0.15.2 --set nfs.path=/srv/nfs/kubedata 

kubectl get po

#create a yaml file: nfsclaim.yaml using vi and copy and paste below lines till 818

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvctest
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi

kubectl create -f nfsclaim.yaml
kubectl get pv
kubeclt get pvc

vi podnfs.yaml
#Copy paste below code
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  volumes:
  - name: myvol
    persistentVolumeClaim:
      claimName: pvctest
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh"]
    args: ["-c", "sleep 600"]
    volumeMounts:
    - name: myvol
      mountPath: /data

    
kubectl create -f podnfs.yaml
kubectl get pods
kubectl describe pod busybox

#In the worker node

ls /srv/nfs/kubedata

Assignment

Assignment: Create a pvc using nfs-client storage class size 200Mi and crate a pod using nginx container and map to /var/www/html of nginx container to newly created pvc



05/06

Lab1
kubectl create -f role.yaml
kubectl get roles
kubectl describe role pod-reader

Lab2:
kubectl create -f rolebind.yaml
 kubectl get rolebindings
 kubectl describe rolebindings read-pods
 
Lab3: 
#Create a private key for your user. In this example, we will name the file employee.key:
openssl genrsa -out employee.key 2048
#Create a certificate sign request employee.csr using the private key you just created (employee.key in this example). Make sure you specify your username and group in the -subj #section
openssl req -new -key employee.key -out employee.csr -subj "/CN=employee/O=test"
#Generate the final certificate employee.crt by approving the certificate sign request, employee.csr, you made earlier. Make sure you substitute the CA_LOCATION placeholder with the location of your cluster CA. In this example, the certificate will be valid for 500 days:
openssl x509 -req -in employee.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out employee.crt -days 500
kubectl config set-credentials employee --client-certificate=employee.crt --client-key=employee.key
#Add a new context with the new credentials for your Kubernetes cluster.
kubectl config set-context employee-context --cluster=kubernetes --namespace=default --user=employee
kubectl config get-contexts
kubectl config use-context employee-context
kubectl get pods
 kubectl delete pods <pod name>
#Change to admin
 kubectl config use-context kubernetes-admin@kubernetes

kubectl delete -f rolebind.yaml
kubectl delete -f role.yaml

------------------------------
Try to Lab 4

Modify the above role.yaml for deployment, deployment api group

kubectl api-resources

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: ["apps"] # "" indicates the core API group
  resources: ["deployments"]
  verbs: ["get", "watch", "list"]                            


kubectl create -f role.yaml
kubeclt create -f rolebind.yaml
kubectl config use-context employee-context
kubectl get deloyment
kubectl delete deployment <name of the deployment>


Lab 5:
    
 kubectl create -f cm.yaml
kubectl get cm
kubectl describe cm game-demo


kubectl create -f cm-pod.yaml
kubectl describe pod configmap-demo-pod
kubectl exec configmap-demo-pod -c demo -it -- /bin/sh
export
ls /config/
exit

Lab 6

kubectl create -f sec.yaml
kubectl get secret
kubectl describe secret database-creds

kubectl create -f secenvpod.yaml
kubectl describe pod php-mysql-app
kubectl exec php-mysql-app -c php-app -it -- /bin/bash
#check the env value
export
exit
#Delete the pod
kubectl delete -f secenvpod.yaml


Project

git clone https://github.com/amitopenwriteup/threetierk8sproject.git
cd threetierk8sproject
#Creating namespace
 kubectl create -f ns.yaml
 kubectl get all -n wordpress
#Creating secrets for db
 kubectl create -f secret.yaml
 kubectl describe secret/db-secret -n wordpress
#Creating pv,pvcfor wp and db
 kubectl create -f pvpvc.yaml
 kubectl create -f pvpvcwp.yaml
 #Creating deployment and serice for db
 kubectl create -f dbdepsvc.yaml
 kubectl get svc -n wordpress
 kubectl get ep -n wordpress
 kubectl get po -n wordpress
# Creting phpadmin for db
 kubectl create -f phpdepsvc.yaml
 kubectl get svc -n wordpress

 #Creating wp
 kubectl create -f wordpressdepsvc.yaml
 kubectl get all -n wordpress

 -->WP website created-->db
--> php admin will be managing db

WP-->Expose to outside world

http://workernodeip:30080
exp: http://10.0.15.2:30081
http://workernodeio:30081

--------------------------
06/06

Lab1 

wget https://get.helm.sh/helm-v3.10.2-linux-amd64.tar.gz
tar -zxvf helm-v3.10.2-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/helm

Step 1: Search for the Prometheus helm chart using the below command

helm search hub Prometheus

#Step 2 adding and upatting the rep

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

#Step 3
helm install prometheus prometheus-community/prometheus

Step 4
kubectl get pods

#describe command run it for pending 

kubectl describe po <name of pending pod>

kubectl get pvc

step  5
kubectl edit pvc prometheus-server
storageClassName: nfs-client
Change the second pvc as well
kubectl edit pvc storage-prometheus-alertmanager-0
storageClassName: nfs-client

Step 6
kubectl get svc
kubectl edit svc prometheus-server

-search for tpe

type: NodePort

kubectl get svc

open in browser

http://workernodeip:portnumber

example

http://10.0.15.2:32416

Lab2: Grafana
Grafana installation
 
helm search hub Grafana
helm repo add grafana https://grafana.github.io/helm-charts 
helm repo update
helm install grafana grafana/grafana


Get the secret/password

   kubectl get secret --namespace default grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
   
Modify the service type to NodePort
kubectl edit svc grafana   
kubectl get svc

Open in browser
http://workdernode ip:portnumber
example
http://10.0.15.2:31114

Login
username : admin
password: output of line 1046


Ingress controller lab:
    
 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml

kubectl get pods --namespace=ingress-nginx

kind: Pod
apiVersion: v1
metadata:
  name: apple-app
  labels:
    app: apple
spec:
  containers:
    - name: apple-app
      image: hashicorp/http-echo
      args:
        - "-text=apple"

---

kind: Service
apiVersion: v1
metadata:
  name: apple-service
spec:
  selector:
    app: apple
  ports:
    - port: 5678 # Default port for image


---

kind: Pod
apiVersion: v1
metadata:
  name: banana-app
  labels:
    app: banana
spec:
  containers:
    - name: banana-app
      image: hashicorp/http-echo
      args:
        - "-text=banana"

---

kind: Service
apiVersion: v1
metadata:
  name: banana-service
spec:
  selector:
    app: banana
  ports:
    - port: 5678 # Default port for image

--------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: demo-localhost
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: demo.localdev.me
    http:
      paths:
        - path: /apple
          backend:
            service:
              name: apple-service
              port:
                number: 5678
          pathType: Prefix
        - path: /banana
          backend:
             service:
               name: banana-service
               port:
                 number: 5678
          pathType: Prefix


kubectl edit validatingwebhookconfiguration ingress-nginx-admission


kubectl create -f ingress.yaml

kubectl get ingress

kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80

$ curl -kL http://demo.localdev.me:8080/apple
apple

$ curl -kL http://demo.localdev.me:8080/banana
banana

$ curl -kL http://demo.localdev.me:8080/notfound
default backend - 404




demo.localdev.me/apple-->Applesvc-->apple pod
demo.localdev.me/banana-->bananasvc-->banan


----------------------------------------------
helm packaging

wget https://get.helm.sh/helm-v3.10.2-linux-amd64.tar.gz
tar -zxvf helm-v3.10.2-linux-amd64.tar.gz

helm create testchart
cd testchart

cd templates
rm -rf *

cd ..

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml >> templates/deployment.yaml

 #create a deployment to expose service
kubectl create deployment nginx --image=nginx

kubectl expose deploy nginx --port 80 --type NodePort --dry-run=client -o yaml > templates/service.yaml

echo "This is first helm chart and it will deploy nginx application" >>templates/NOTES.txt

cd ..

helm lint ./testchart

#delete the deployment

kubectl delete deployment nginx
kubectl delete svc nginx

12)#Do the dry run
helm install testchart ./testchart --dry-run

13) #install

helm install testchart ./testchart
#Check service and deployment
kubectl get deployment
kubectl get svc

14) #check
helm list

15) #package
helm package testchart/
#Successfully packaged chart and saved it to: /root/testchart-0.1.0.tgz
16) #Uninstall
helm uninstall testchart
#Check service and deployment
kubectl get svc
kubectl get deployment



09/06

From the cotnrol node (where is ansible installed)

Lab1

ssh-keygen -t rsa

ssh-copy-id root@secondvmip

Example

ssh-copy-id root@10.0.15.2

PAssword: root123


Verify

ssh root@managenode ip

example

ssh root@10.0.15.2


#Login without password then we are good

exit

------------------------------------

ini or yaml

Lab2:
creation of inventory.ini
    
[local]
localhost ansible_connection=local
[remote]
10.0.15.2 ansible_user=root ansible_connection=ssh

Run below command

ansible -i inventory.ini all -m ping
ansible -i inventory.ini local -m ping
ansible -i inventory.ini remote -m ping


Lab3

Create playbook: lab1.yaml

- name: ping task
  hosts: all
  tasks:
  - name: ping testing
    ansible.builtin.ping:
  - name: pring msg
    ansible.builtin.debug:
      msg: hello

ansible-playbook -i inventory.ini lab1.yaml

Lab 4:
Create playbook: lab2.yaml

- name: ping task
  hosts: all
  tasks:
  - name: ping testing
    ansible.builtin.ping:
- name: second play
  hosts: remote
  tasks:
  - name: pring msg
    ansible.builtin.debug:
      msg: hello

ansible-playbook -i inventory.ini lab2.yaml


Lab 5:

---
- name: Add remote host dynamically
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Add remote host to inventory
      ansible.builtin.add_host:
        name: 10.0.15.2
        groups: remote
        ansible_user: root
        ansible_connection: ssh

- name: Ping remote hosts
  hosts: all
  gather_facts: false
  tasks:
    - name: Ping testing
      ansible.builtin.ping:

- name: Second play - print message
  hosts: remote
  gather_facts: false
  tasks:
    - name: Print message
      ansible.builtin.debug:
        msg: "Hello from dynamically added host!"


ansible-playbook lab3.yaml


Lab 6: lab4 .yaml

Manage node:

adduser testuser
passwd testuser

vi /etc/sudoers

testuser ALL=(ALL) NOPASSWD: ALL

esc+:wq!

go to control/master node
modify invientory.ini

instead of root give testuser

10.0.15.2 ansible_user=testuser ansible_connection=ssh

touch index.html

- name: install package
  hosts: all
  become: true
  tasks:
    - name: dnfinstaller
      ansible.builtin.dnf:
        name: httpd
        state: present
    - name: copy file
      copy:
        src: index.html
        dest: /var/www/html/index.html
        owner: root
        group: root
        mode: '0644'

ansible-playbook -i inventory.ini lab4.yaml


lab  7

ansible-vault create vault.yml

ansible_user_password: "mention your testuser password"

Modify inventory.ini

10.0.15.2 ansible_user=testuser ansible_connection=ssh ansible_password="{{ ansible_user_password }}

#Create another file lab5.yaml

- name: install package
  hosts: all
  become: true
  vars_files:
    - vault.yml
  tasks:
    - name: dnfinstaller
      ansible.builtin.dnf:
        name: httpd
        state: present
    - name: copy file
      copy:
        src: index.html
        dest: /var/www/html/index.html
        owner: root
        group: root
        mode: '0644'

ansible-playbook -i inventory.ini lab5.yaml --ask-vault-pass


lab 8

ansible-galaxy role init test

create account on galaxy

https://galaxy.ansible.com/ui/

------------------------------------------
10/06

Lab1

rm -rf test

1) Run below command

 ansible-galaxy role init test

2) copy the section, to test/tasks/main.yaml

---
- name: dnfinstaller
  ansible.builtin.dnf:
     name: httpd
     state: present
- name: copy file
  copy:
     src: files/index.html
     dest: /var/www/html/index.html
     owner: root
     group: root
     mode: '0644'
3) create a index.html file under test/files/

touch test/files/index.html

4) create a playbook lab5.yaml, outside of test folder

---
- hosts: all
  become: true
  vars_files:
    - vault.yaml
  roles:
   - test
5) Run the command
ansible-playbook -i inventory.ini lab5.yaml  --ask-vault-pass
   


ansible-galaxy role install citizennone.rkhunter

LAb 2
How to publish the role to ansible-galaxy


go to role folder

cd test
git init
git add .
git commit -m "add"

https://www.openwriteup.com/?page_id=1081


Lab 3
1)https://galaxy.ansible.com/ui/
2) Login using github
3) go to galaxy home page, collection-->api-token
4) ansible-galaxy import <GitHub user name> <repo name on github> --token <token id>
example

ansible-galaxy import amitopenwriteup myansiblerole --token 208fb92ea9dfe552476822dd01a8dd1b02ea13e7

Lab 4:
1) Create a lab6.yaml file

---
- hosts: all
  become: true
  vars_files:
    - vault.yaml
  tasks:
    - name: Install security updates
      ansible.builtin.dnf:
        name: "{{ item }}"
        state: latest
      loop:
        - httpd
        - openssh
      ignore_errors: yes 
    - name: Check if docker is installed
      ansible.builtin.command: docker --version
      register: output
      ignore_errors: yes    
    - ansible.builtin.debug:
        var: output
    - name: Install docker
      ansible.builtin.dnf:
        name: docker.io
        state: present
      when: output.failed
        

2) run 
ansible-playbook -i inventory.ini lab6.yaml --ask-vault-pass

Lab 5;
---
- name: Install Docker based on OS family
  hosts: all
  become: true
  vars:
    docker_packages_redhat:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-buildx-plugin
      - docker-compose-plugin
    docker_packages_debian:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-buildx-plugin
      - docker-compose-plugin
    docker_repo_url_redhat: https://download.docker.com/linux/centos/docker-ce.repo
    docker_repo_url_debian: https://download.docker.com/linux/ubuntu
    docker_repo_key_url_debian: https://download.docker.com/linux/ubuntu/gpg
  tasks:
    - name: Install prerequisites on RedHat
      dnf:
        name: dnf-plugins-core
        state: present
      when: ansible_os_family == "RedHat"
    - name: Add Docker CE repo on RedHat
      get_url:
        url: "{{ docker_repo_url_redhat }}"
        dest: /etc/yum.repos.d/ -ce docker.repo
      when: ansible_os_family == "RedHat"
      notify: Restart Docker
    - name: Install Docker packages on RedHat
      dnf:
        name: "{{ docker_packages_redhat }}"
        state: present
      when: ansible_os_family == "RedHat"
      notify: Restart Docker
    - name: Install prerequisites on Debian
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
        state: present
      when: ansible_os_family == "Debian"
    - name: Add Docker GPG key on Debian
      apt_key:
        url: "{{ docker_repo_key_url_debian }}"
        state: present
      when: ansible_os_family == "Debian"
    - name: Add Docker repository on Debian
      apt_repository:
        repo: "deb [arch=amd64] {{ docker_repo_url_debian }} {{ ansible_lsb.codename }} stable"
        state: present
      when: ansible_os_family == "Debian"
      notify: Restart Docker
 
    - name: Install Docker packages on Debian
      apt:
        name: "{{ docker_packages_debian }}"
        state: present
      when: ansible_os_family == "Debian"
      notify: Restart Docker
 
    - name: Enable and start Docker service
      systemd:
        name: docker
        enabled: true
        state: started
 
  handlers:
    - name: Restart Docker
      systemd:
        name: docker
        state: restarted
 
Lab 6:
    
Write a  playbook

    - name: uninstall docker
      ansible.builtin.dnf:
        name: docker.io
        state: absent
        
        
Try lab 5 again.

Lab 7:
    
 ---
- name: Ensure SELinux is set to enforcing mode
  hosts: all
  vars_files
    - vault.yml
  become: true
  gather_facts: false
  tasks:
    - name: Ensure SELinux is set to enforcing mode
      ansible.builtin.lineinfile:
        path: /etc/selinux/config
        regexp: '^SELINUX='
        line: SELINUX=enforcing
        
        
---cross verify

cat /etc/selinux/config        
        
11/06/

Collection lab1:
#Install it first (on your control node):
ansible-galaxy collection install community.docker

#Also make sure Python Docker SDK is installed on the managed node:

sudo dnf install -y python3-pip
pip3 install docker

Ansible Playbook Using community.docker
- name: Deploy PostgreSQL using Docker collection
  hosts: remote
  vars_files:
    - vault.yml
  become: true
  tasks:
    - name: Create a Docker volume for PostgreSQL data
      community.docker.docker_volume:
        name: pgdata
    - name: Pull the official PostgreSQL image
      community.docker.docker_image:
        name: postgres
        source: pull
    - name: Run PostgreSQL container
      community.docker.docker_container:
        name: postgres
        image: postgres
        state: started
        restart_policy: always
        ports:
          - "5432:5432"
        env:
          POSTGRES_PASSWORD: mysecretpassword
          POSTGRES_USER: myuser
          POSTGRES_DB: mydb
        volumes:
          - pgdata:/var/lib/postgresql/data
 

Lab 2:
ansible-galaxy collection install community.mysql

- name: Simulate RDS MySQL setup on CentOS
  hosts: all
  become: yes
  vars:
    mysql_root_password: "RootPass123!"
    db_name: "testdb"
    db_user: "dbuser"
    db_user_password: "UserPass123!"
  tasks:
    - name: Ensure Python 3 and pip are installed
      yum:
        name:
          - python3
          - python3-pip
        state: present
    - name: Install PyMySQL module for Python 3
      pip:
        name: PyMySQL
        executable: pip3
    - name: Install MariaDB server
      yum:
        name: mariadb-server
        state: present
    - name: Start and enable MariaDB service
      service:
        name: mariadb
        state: started
        enabled: true
    - name: Set MySQL root password
      mysql_user:
        name: root
        host_all: yes
        password: "{{ mysql_root_password }}"
        login_unix_socket: /var/lib/mysql/mysql.sock
      ignore_errors: true
    - name: Create a MySQL database
      mysql_db:
        name: "{{ db_name }}"
        state: present
        login_user: root
        login_password: "{{ mysql_root_password }}"
    - name: Create a MySQL user with privileges
      mysql_user:
        name: "{{ db_user }}"
        password: "{{ db_user_password }}"
        priv: "{{ db_name }}.*:ALL"
        host: "%"
        state: present
        login_user: root
        login_password: "{{ mysql_root_password }}"
        
        
        
        MAster;
        mysql   --user=root   --password
        
        #pasword is RootPass123!

sudo firewall-cmd --permanent --add-port=3306/tcp
        
        
        
[client]
user=root
password=RootPass123!
        
chmod 600 /root/.my.cnf        
        
        
kubectl get nodes

systemctl restart containerd
systemctl restart kubelet

        
        
        
        
