Lab 1
 
Jenkins master setup: https://www.jenkins.io/doc/book/installing/linux/#red-hat-centos
 
sudo wget -O /etc/yum.repos.d/jenkins.repo  https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
yum update

# Add required dependencies for the jenkins package
sudo yum install fontconfig java-21-openjdk
sudo yum install jenkins
sudo systemctl daemon-reload
systemctl status jenkins
systemctl enable jenkins
systemctl start jenkins
systemctl status jenkins
 

------------------------
Lab2
1) In the browser : localhost:8080

2) Go to terminal and type

cat /var/lib/jenkins/secrets/initialAdminPassword

3) get the password and login

4) Install suggested plugin

 
 Lab 3:
Lab 2 https://www.openwriteup.com/?page_id=1115

node {
// Define the label of the agent where you want to run the pipeline
//label 'mylabel'
stage('Checkout') {
// Checkout code from the Git repository
sh 'echo checking out'
}
stage('Build') {
// Build the Java application (replace with your build commands)
sh 'java --version'
}
stage('Deploy') {
// Deploy the application (replace with your deployment commands)
sh 'echo "Deploying the application"'
}
}


Plugin: pipeline stage view

-----------------

Lab 4 : Decalartive pipeline :https://www.openwriteup.com/?page_id=1115 (Lab 3)

pipeline {
agent any
stages {
  stage('Checkout') {
    steps {
// Checkout code from the Git repository
     sh 'echo checking out'
   }
  }
stage('Build') {
  steps {
// Build the Java application (replace with your build commands)
  sh 'javac -version'
 }
}
stage('Deploy') {
  steps {
  // Deploy the application (replace with your deployment commands)
   sh 'echo "Deploying the application"'
   }
  }
 } 
} 

Lab 5: Assignment
    
Go in your git repo “myrepo”
Create a “Jenkinsfile”
Copy the code from lab 3
checkin the code to github (using git add,commit and push
Create a new pipeline and scripting block dropdown, select “pipeline with SCM”
Mention your git repo and save .
Run the build


--------------------------------------
27/05

git add,commit and push using  ssh

https://www.openwriteup.com/?page_id=1075

Do the git init, git add, git commit
 

LAb2

https://www.openwriteup.com/?page_id=1081

Lab3:

dnf install maven -y

Lab 1: https://www.openwriteup.com/?page_id=1135

Lab 2: https://www.openwriteup.com/?page_id=1135

Lab 4: https://www.openwriteup.com/?page_id=1135

Lab https://www.openwriteup.com/?page_id=1073 (lab 17)


---------------------------
28/05

https://www.sonarsource.com/products/sonarcloud/

Lab https://www.openwriteup.com/?page_id=1223
 
 
vi /etc/sudoers

jenkins ALL=(ALL) NOPASSWD: ALL
 
 pipeline {
agent any

stages {
stage('chckout scm') {
steps {
checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/hellokaton/java11-examples.git']])
}
}
stage('Compiling and Running Test Cases') {
steps {
sh 'mvn clean'
sh 'mvn compile'
sh 'mvn test'
}
}
stage('Generating a Cucumber Reports') {
steps {
script {
// Run Cucumber tests and generate reports
sh 'mvn verify'
}
}
}
stage('Creating Package') {
steps {
sh 'mvn package'
}
}
stage('adding genrerate report'){
steps {
sh 'mvn verify'
}
}
stage('Install sonarqube cli') {
steps {
// Step to install SonarQube CLI
sh 'sudo wget -O sonar-scanner.zip https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-5.0.1.3006-linux.zip'
sh 'sudo unzip -o -q sonar-scanner.zip'
sh 'sudo rm -rf /opt/sonar-scanner'
sh 'sudo mv --force sonar-scanner-5.0.1.3006-linux /opt/sonar-scanner'
sh 'sudo sh -c \'echo "#/bin/bash \nexport PATH=\\\"$PATH:/opt/sonar-scanner/bin\\\"" >/etc/profile.d/sonar-scanner.sh\''
sh 'sudo chmod +x /opt/sonar-scanner/bin/sonar-scanner'
sh '. /etc/profile.d/sonar-scanner.sh'
}
}

stage('Analyzing Code Quality') {
steps {
// Step to analyze code quality with SonarQube
sh '/opt/sonar-scanner/bin/sonar-scanner -Dsonar.projectKey=owtest23_myproj -Dsonar.organization=owtest23 -Dsonar.qualitygate.wait=true -Dsonar.qualitygate.timeout=300 -Dsonar.sources=src/main/java/ -Dsonar.java.binaries=target/classes -Dsonar.host.url=https://sonarcloud.io -Dsonar.login=cd718b63ea3b0b88b10541ff23b08430789260e1'
}
}
}
}
---------------------------
install dcoekr 
LAb 1 : https://www.openwriteup.com/?page_id=785

Lab2:

docker search nginx
docker pull nginx
docker images

docker pull mysql:innovation
docker images


docker pull mysql:innovation
docker images

#To check the running container
 
docker ps

docker run -d --name mycontainer nginx

docker ps

docker stop mycontainer

docker ps

docker ps -a

docker start mycontainer

docker ps

-------------------------------

docker inspect mycontainer

'---------------
29/05

Lab

dokken/centos-8

docker pull dokken/centos-8

docker images
docker run -dit --name centos dokken/centos-8 bash
docker ps --nam
docker exec -it centos ip addr
docker exec -it centos ps

windowsn containre
docker pull mcr.microsoft.com/windows/nanoserver:ltsc2025


--------------------------------------

Lab 2

1) docker pull debian:stable
2) docker run -dit --name testdeb debian:stable
3) docker exec -it testdeb ls /var/www/html

#error file not found

4) vi dockerfile and copy below content

FROM debian:stable
LABEL authors="amit"
RUN apt-get update && apt-get install -y --force-yes apache2
EXPOSE 80

5) docker build -t mydeb .
6) docker images
7) docker run -dit --name testweb mydeb
8) docker exec -it testweb ls /var/www/html

Lab3:

docker pull ubuntu:latest
docker run -dit --name addcopy ubuntu:latest
docker exec -it addcopy ls /app

touch file.txt
vi dockerfileadd and copy below code, save it

FROM ubuntu:latest
# Set a working director inside container
WORKDIR /app
#Copy file from current directory
COPY file.txt /app/
#ADD a directory from the host
#ADD /home/lab-user/amit /app/
ADD https://github.com/amitopenwriteup/my_proj/blob/master/k8s-setup.txt /app/
#Till this point copy it in docker file

docker build -t ubuntuimage -f dockerfileadd  .

docker images

docker run -dit --name addcopytest ubuntuimage
docker exec -it addcopytest ls /app

-----------------------

CMD

#Create date.sh file and copy below script

#!/bin/sh
echo `date` $@ >> log.txt;
cat log.txt;
#save date.sh script

chmod 777 date.sh

#Create a dockercmd file

FROM alpine
ADD date.sh /
CMD ["/date.sh", "container started"]

docker build -t cmdtest -f dockercmd .
docker run cmdtest


-------------------


30/05

https://www.openwriteup.com/?page_id=825 (LAb1 to Lab4 ) : RUN CMD and ENTRYPOINT

LAb2:
signup using github

https://hub.docker.com/



Lab3:
https://www.openwriteup.com/?page_id=838

Push the image

Volume lab:

docker stop registry
docker rm registry

# Refresh the broswer, it will give error page not found

http://localhost:5000/v2/_catalog

docker run -d -p 5000:5000 --restart=always --name registry registry:2

AGain open the browser the below link and check
http://localhost:5000/v2/_catalog

docker stop registry
docker rm registry

----
create a volume

docker volume create datavol
docker inspect datavol
docker run -d -p 5000:5000 --restart=always -v datavol:/var/lib/registry  --name registry registry:2
docker push localhost:5000/my-image
docker stop registry
docker rm registry

   docker run -d -p 5000:5000 --restart=always -v datavol:/var/lib/registry  --name registry registry:2
AGain open the browser the below link and check
http://localhost:5000/v2/_catalog

docker stop registry
docker rm registry

----------map your own volume

mkdir /root/test

docker run -d -p 5000:5000 --restart=always -v /root/test:/var/lib/registry  --name registry registry:2

docker push localhost:5000/my-image
docker stop registry
docker rm registry

docker run -d -p 5000:5000 --restart=always -v /root/test:/var/lib/registry  --name registry registry:2

AGain open the browser the below link and check
http://localhost:5000/v2/_catalog


Lab 4:

#Create the alpine-net network. You do not need the --driver bridge flag since it's the default, but this example shows how to specify it.

docker network create --driver bridge alpine-net

docker network ls

#inspect

docker network inspect alpine-net

Create your four containers. Notice the --network flags. You can only connect to one network during the docker run command, so you need to use docker network connect afterward to connect alpine4 to the bridge network as well.

docker run -dit --name alpine1 --network alpine-net alpine ash

docker run -dit --name alpine2 --network alpine-net alpine ash

docker run -dit --name alpine3 alpine ash

docker run -dit --name alpine4 --network alpine-net alpine ash

docker network connect bridge alpine4

#check all container

docker container ls

docker network inspect bridge


docker network inspect alpine-net


#Ping test

docker container attach alpine1
ping -c 2 alpine2
ping -c 2 alpine4

#It should fail for alpine 3, why?
ping -c 2 alpine3

#Stop and delete

docker container stop alpine1 alpine2 alpine3 alpine4

docker container rm alpine1 alpine2 alpine3 alpine4

docker network rm alpine-net


LAb 5: cicd for dockerfile

https://www.openwriteup.com/?page_id=1350


sudo usermod -a -G docker jenkins
sudo chown jenkins:docker /var/run/docker.sock
sudo chmod 660 /var/run/docker.sock

From Manage plugin install docker pipeline plugin
Create a pipeline And select option “pipeline with scm”
Provide this git link
https://github.com/amitopenwriteup/cicd.git
Branch : master
Jenkinsfile name: Jenkinsfile


---------------------
02/06

Install docker in second vm

install dcoekr 
LAb 1 : https://www.openwriteup.com/?page_id=785


yum remove podman*

yum remove runc* containerd* buildah*

sudo yum install -y yum-utils

sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

systemctl enable docker


systemctl start docker

systemctl status docker

Lab 2:

1) note down the ip address of second vm

ip addr|grep ens

2) First .1 linux vm

ssh root@ipaddrsss
#password is root123

Run step1 in both the linux vm

https://www.openwriteup.com/?page_id=866

step2 : Execute in both vms

https://v1-31.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

step 3:
 master .1

sudo firewall-cmd --permanent --add-port=6443/tcp 
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp 
sudo firewall-cmd --permanent --add-port=10251/tcp 
sudo firewall-cmd --permanent --add-port=10259/tcp 
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=4789/udp
sudo firewall-cmd --reload

worker node /slave 2
sudo firewall-cmd --permanent --add-port=179/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=30000-32767/tcp
sudo firewall-cmd --permanent --add-port=4789/udp
sudo firewall-cmd --reload
Run on both nodes

swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

sh -c "containerd config default > /etc/containerd/config.toml"
sed -i 's/ SystemdCgroup = false/ SystemdCgroup = true/' /etc/containerd/config.toml
systemctl restart containerd.service
systemctl restart kubelet.service
systemctl enable kubelet.service


step 4 only on master

kubeadm init --pod-network-cidr=10.244.0.0/16


Step5: Runs only in master node
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install the network plugin in master node
step 6: only in master
 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

step 7 verify

sudo firewall-cmd --reload

kubectl get nodes


step 8 on worker node

on the master

kubeadm token create --print-join-command


outp run the join command run in workernode
---------------------

Lab9
 create pod
vi testpod.yaml

copy and paste:
apiVersion: v1
kind: Pod
metadata:
  name: testpod
spec:
  containers:
  - name: firstcontainer
    image: nginx            


kubectl create -f testpod.yaml

kubectl get pods
kubeclt describe pods testpod
kubectl get pods -o wide

Check taint
kubectl get nodes

kubectl describe nodes localhost.localdomain|grep Taint


03/06

kubectl get nodes
kubectl get pods

Lab 1:

rm -rf k8s

 git clone https://github.com/amitopenwriteup/k8s.git

cd k8s
kubectl create -f multicont.yaml
kubectl get pods
kubectl describe pod multi-container-pod
kubectl exec multi-container-pod -c main-container -- date
kubectl exec multi-container-pod -c sidecar-container -- ls
kubectl delete pod multi-container-pod

LAb 2
kubectl create ns labs

kubectl describe ns labs

kubectl get ns labs -o yaml

kubectl delete ns labs

Lab 3


cd k8s
cat devpod.yaml
kubectl create ns dev
kubectl create -f devpod.yaml
kubectl get pods --namespace=dev
kubectl describe pod busybox-pod --namespace=dev
kubectl delete pod busybox-pod --namespace=dev


Lab: 4

 kubectl create -f repc.yaml
kubectl get rc
kubectl describe rc hello-dep
kubectl get pods --show-labels
kubectl get pods
kubectl delete pods <provide one of the replica pod name?
kubectl get pods --show-labels
# for deleting rc
kubectl delete -f repc.yaml


Lab 5


kubectl create -f dep.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment nginx-deployment
#For deleting the deployment
kubectl delete deployment nginx-deployment


Lab 6

kubectl create -f ru.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment hello-rcp
# update the image
 kubectl set image deployment/hello-rcp hello-dep=nginx:1.16.1
# Check rollout status
kubectl rollout status deployment/hello-rcp
#Rollout history
kubectl rollout history deployment/hello-rcp
#roll back
 kubectl rollout undo deployment/hello-rcp --to-revision=1
#scaleup
 kubectl scale deployment/hello-rcp --replicas=10

kubectl rollout history deployment hello-rcp --revision=0
kubectl rollout history deployment hello-rcp --revision=2
#delete
 kubectl delete deployment.apps/hello-rcp

LAb7 

Recreate strategy

kubectl create -f rc.yaml
kubectl get deployment
kubectl get pods
kubectl get rs
kubectl describe deployment hello-dep
# update the image
kubectl set image deployment/hello-dep hello-dep=nginx:1.16.1
# Check rollout status
kubectl rollout status deployment/hello-dep
#Rollout history
kubectl rollout history deployment/hello-dep
#roll back
kubectl rollout undo deployment/hello-dep --to-revision=1
#scaleup
kubectl scale deployment/hello-dep --replicas=10
#delete
kubectl delete deployment.apps/hello-dep


--------------------
04/06

git clone https://github.com/amitopenwriteup/k8s.git
cd k8s

Lab1

kubectl create -f svc.yaml
kubectl get svc
kubectl get ep
kubectl describe svc nginx-service-rc
kubectl create -f dep.yaml
kubectl get pods -o wide
kubectl get ep

##Delete the service
kubectl delete -f svc.yaml
kubectl detete -f dep.yaml


-------------LAb2

kubectl create -f dep.yaml
kubectl create -f Nodeport.yaml
kubectl get svc
kubectl get ep

#Note down the worker node (join)

ip addr|grep ens

#Open the ip in the broser

http://ipaddr:30007

#Delete the nodeport service
kubectl delete svc my-service


Lab 3-5

kubectl create -f pv.yaml
kubectl get pv
kubectl describe pv pv-volume-2

kubectl create -f pvc.yaml
kubectl get pvc
kubectl get pv


#Modify the pvpod.yaml, claim name: pvc-claim-3

kubectl create -f pvpod.yaml
kubectl get pod
kubectl exec mypod -it -c myfrontend -- /bin/bash
cd /var/www/html
#create a file using below cmd
touch a
#exit from the pod:
exit
#check the file created in worker node
ls /mnt/data/
--------------

Lab 6 : Dyanmic provisioning

#NFS storage 

#For rocky linux
sudo dnf install -y nfs-utils

#Making a directory in host machine where PersistentVolumeClaim (PVC) will be created

sudo mkdir /srv/nfs/kubedata -p
#Now we will edit the exports file and add the directory 
#which we created earlier step in order to export it into the remote machine

sudo vi /etc/exports

/srv/nfs/kubedata    *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)

sudo systemctl enable nfs-server
sudo systemctl start nfs-server
sudo systemctl status nfs-server
exportfs

sudo exportfs -rav


Master only run below command

dnf install nfs-utils -y

#Install NFS client packages in nodes
wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz
tar -zxvf helm-v3.13.2-linux-amd64.tar.gz
mv linux-amd64/helm /bin/helm

#Add the stable repository in Helm repo and update
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner

helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=10.0.15.2 --set nfs.path=/srv/nfs/kubedata 

kubectl get po

#create a yaml file: nfsclaim.yaml using vi and copy and paste below lines till 818

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvctest
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi

kubectl create -f nfsclaim.yaml
kubectl get pv
kubeclt get pvc

vi podnfs.yaml
#Copy paste below code
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  volumes:
  - name: myvol
    persistentVolumeClaim:
      claimName: pvctest
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh"]
    args: ["-c", "sleep 600"]
    volumeMounts:
    - name: myvol
      mountPath: /data

    
kubectl create -f podnfs.yaml
kubectl get pods
kubectl describe pod busybox

#In the worker node

ls /srv/nfs/kubedata

Assignment

Assignment: Create a pvc using nfs-client storage class size 200Mi and crate a pod using nginx container and map to /var/www/html of nginx container to newly created pvc





































